{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To obtain submodel_four function's 1st input variable, global_parameters.pt\n",
    "\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Neural Network Model\n",
    "class SentimentAnalysisModel_global(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentAnalysisModel_global, self).__init__()\n",
    "        self.fc1 = nn.Linear(10000, 2)\n",
    "        self.fc2 = nn.Linear(2, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # Replace BatchNorm with GroupNorm\n",
    "        self.groupnorm = nn.GroupNorm(1, 2)  # GroupNorm with 1 group and 2 channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.groupnorm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting fixed seed for reproducibility of the random weights and biases\n",
    "torch.manual_seed(19)\n",
    "\n",
    "# Initialized global model with random weights and biases\n",
    "model_global = SentimentAnalysisModel_global()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[ 0.0094, -0.0060,  0.0076,  ..., -0.0070,  0.0011,  0.0049],\n",
       "                      [-0.0077, -0.0098, -0.0039,  ..., -0.0005, -0.0024, -0.0059]])),\n",
       "             ('fc1.bias', tensor([0.0017, 0.0014])),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.5814,  0.0490],\n",
       "                      [-0.1220, -0.4694],\n",
       "                      [ 0.4106, -0.6040]])),\n",
       "             ('fc2.bias', tensor([-0.5295,  0.3521,  0.5361])),\n",
       "             ('groupnorm.weight', tensor([1., 1.])),\n",
       "             ('groupnorm.bias', tensor([0., 0.]))])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See the object containing the randomn weights and biases for this global model\n",
    "model_global.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save global model weights and biases in file global_parameters.pt\n",
    "torch.save(model_global.state_dict(), 'global_parameters.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To obtain submodel_four function's 2nd input variable, synth_local_text_df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "node1_df = pd.read_csv('node1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\leo_c\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure the necessary NLTK components are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "def build_markov_model(data, n_gram=2):\n",
    "    model = {}\n",
    "    for sentence in data:\n",
    "        sentence = word_tokenize(sentence.lower())\n",
    "        for i in range(len(sentence) - n_gram):\n",
    "            gram = tuple(sentence[i:i + n_gram])\n",
    "            next_word = sentence[i + n_gram]\n",
    "            if gram not in model:\n",
    "                model[gram] = {}\n",
    "            if next_word not in model[gram]:\n",
    "                model[gram][next_word] = 0\n",
    "            model[gram][next_word] += 1\n",
    "    return model\n",
    "\n",
    "def generate_sentence(model, max_length=20):\n",
    "    sentence = []\n",
    "    n_gram = list(model.keys())[random.randint(0, len(model) - 1)]\n",
    "    sentence.extend(n_gram)\n",
    "    for _ in range(max_length - len(n_gram)):\n",
    "        last_gram = tuple(sentence[-len(n_gram):])\n",
    "        if last_gram in model:\n",
    "            next_words = model[last_gram]\n",
    "            next_word = random.choices(list(next_words.keys()), weights=next_words.values())[0]\n",
    "            sentence.append(next_word)\n",
    "        else:\n",
    "            break\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "def submodel_three(dataset, seed=13):   # RANDOM SEEDS 13 & 18 GIVES SYNTHESIZED CAPTIONS WITH SENTIMENT = GOOD\n",
    "    # Set the random seed for reproducibility\n",
    "    random.seed(seed)\n",
    "\n",
    "    captions = dataset['Caption'].tolist()\n",
    "    markov_model = build_markov_model(captions)\n",
    "\n",
    "    synthesized_captions = []\n",
    "    for _ in range(5):\n",
    "        synthesized_captions.append(generate_sentence(markov_model))\n",
    "\n",
    "    return pd.DataFrame(synthesized_captions, columns=['Caption'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crouching down next to two water bottles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coat and glass table</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fcx tutorial video 4 - volumetric smoke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hands on her face and her mouth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crouching down next to two water bottles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Caption\n",
       "0  crouching down next to two water bottles\n",
       "1                      coat and glass table\n",
       "2   fcx tutorial video 4 - volumetric smoke\n",
       "3           hands on her face and her mouth\n",
       "4  crouching down next to two water bottles"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synth_local_text_df = submodel_three(node1_df)\n",
    "synth_local_text_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Node</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>VideoID</th>\n",
       "      <th>Caption</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>node1</td>\n",
       "      <td>good</td>\n",
       "      <td>ockU1Ij8OEs</td>\n",
       "      <td>the open document in an excel document</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Node Sentiment      VideoID                                 Caption\n",
       "0  node1      good  ockU1Ij8OEs  the open document in an excel document"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### To obtain submodel_four function's 3rd input variable, node1_incremental_1_df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "node1_incremental_1_df = pd.read_csv('node1_incremental_1.csv')\n",
    "node1_incremental_1_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from opacus import PrivacyEngine\n",
    "from river import preprocessing\n",
    "from river.anomaly import HalfSpaceTrees\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Function to check and balance the dataset\n",
    "def balance_dataset(df):\n",
    "    sentiment_counts = df['Sentiment'].value_counts()\n",
    "    if len(sentiment_counts) > 0 and any(sentiment_counts != sentiment_counts.iloc[0]):\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        df_balanced, _ = ros.fit_resample(df, df['Sentiment'])\n",
    "        return df_balanced\n",
    "    return df\n",
    "\n",
    "# Text preprocessing steps\n",
    "def preprocess_text(text):\n",
    "    # Text cleaning\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Normalization (using lemmatization here)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    normalized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Stop words removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in normalized_tokens if token not in stop_words]\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "# Function to create embeddings\n",
    "def create_embeddings(sentences):\n",
    "    model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "    return model\n",
    "\n",
    "# Function to do padding\n",
    "def pad_or_truncate_sequences(sequences, fixed_length, vector_size):\n",
    "    # Initialize a zero-filled 3D array: number of sequences x fixed_length x vector_size\n",
    "    adjusted_sequences = np.zeros((len(sequences), fixed_length, vector_size))\n",
    "    \n",
    "    for i, sequence in enumerate(sequences):\n",
    "        sequence_length = len(sequence)\n",
    "        if sequence_length > fixed_length:\n",
    "            # Truncate the sequence\n",
    "            adjusted_sequences[i, :, :] = np.array(sequence[:fixed_length])\n",
    "        else:\n",
    "            # Pad the sequence with zeros\n",
    "            adjusted_sequences[i, :sequence_length, :] = np.array(sequence)\n",
    "    \n",
    "    return adjusted_sequences\n",
    "\n",
    "# Custom Neural Network Model\n",
    "class SentimentAnalysisModel_global(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SentimentAnalysisModel_global, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2)\n",
    "        self.fc2 = nn.Linear(2, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        # Replace BatchNorm with GroupNorm\n",
    "        self.groupnorm = nn.GroupNorm(1, 2)  # GroupNorm with 1 group and 2 channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.groupnorm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "def submodel_four(global_parameters_path, synth_local_text_df, node1_incremental_1_df):\n",
    "    # Adjusting sequences to the fixed input size of 100\n",
    "    # Assuming each word vector from Word2Vec is of size 100\n",
    "    vector_size = 100\n",
    "    fixed_input_size = 100\n",
    "    modified_input_size = fixed_input_size * vector_size  # 10000\n",
    "    \n",
    "    # Load model with pretrained parameters\n",
    "    model = SentimentAnalysisModel_global(input_size=modified_input_size)\n",
    "    model.load_state_dict(torch.load(global_parameters_path))\n",
    "    model.eval()\n",
    "\n",
    "    # Step 1: Predict sentiment for synth_local_text_df\n",
    "    synth_local_text_df['processed_caption'] = synth_local_text_df['Caption'].apply(preprocess_text)\n",
    "\n",
    "    # Create a Word2Vec model from all sentences\n",
    "    sentences = synth_local_text_df['processed_caption'].tolist()\n",
    "    word2vec_model = create_embeddings(sentences)\n",
    "\n",
    "    # Convert sentences to sequences of vectors\n",
    "    vectorized_synth_local = [[word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv] for sentence in sentences]\n",
    "\n",
    "    # Pad or truncate the sequences\n",
    "    adjusted_sequences = pad_or_truncate_sequences(vectorized_synth_local, fixed_input_size, vector_size)\n",
    "    adjusted_sequences = adjusted_sequences.reshape(len(adjusted_sequences), -1)\n",
    "\n",
    "    # Convert to tensor and predict\n",
    "    synth_local_tensor = torch.tensor(adjusted_sequences, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(synth_local_tensor)\n",
    "        predicted_labels = torch.argmax(predictions, dim=1)\n",
    "\n",
    "    synth_local_text_df['Local_sentiment'] = predicted_labels.numpy()\n",
    "    pred_synth_local_text_df = synth_local_text_df[['Caption', 'Local_sentiment']]\n",
    "\n",
    "    # Step 2: Create recommend_df\n",
    "    recommend_df = pred_synth_local_text_df[pred_synth_local_text_df['Local_sentiment'] == 1]  # Assuming 1 corresponds to 'good'\n",
    "\n",
    "    # Convert numeric Local_sentiment values back to string labels\n",
    "    sentiment_label_mapping = {0: 'bad', 1: 'good', 2: 'neutral'}\n",
    "    bgn_recommend_df = recommend_df.copy()\n",
    "    bgn_recommend_df['Local_sentiment'] = bgn_recommend_df['Local_sentiment'].replace(sentiment_label_mapping)\n",
    "\n",
    "    # Step 3: Create local_node1_incremental_1_df\n",
    "    local_node1_incremental_1_df = node1_incremental_1_df[['Caption', 'Sentiment']].rename(columns={'Sentiment': 'Local_sentiment'})\n",
    "\n",
    "    # Step 4: Outlier detection\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    hst = HalfSpaceTrees()\n",
    "\n",
    "    # Convert sentiment labels to numeric if they are not already\n",
    "    sentiment_mapping = {'good': 1, 'bad': 0, 'neutral': 2}\n",
    "    pred_synth_local_text_df['Local_sentiment'] = pred_synth_local_text_df['Local_sentiment'].replace(sentiment_mapping)\n",
    "\n",
    "    numerical_features = pred_synth_local_text_df[['Local_sentiment']]\n",
    "\n",
    "    # Train the outlier detector\n",
    "    for _, row in numerical_features.iterrows():\n",
    "        row_dict = row.to_dict()\n",
    "        scaler.learn_one(row_dict)\n",
    "        vec = scaler.transform_one(row_dict)\n",
    "        hst.learn_one(vec)\n",
    "\n",
    "    # Prepare the data from local_node1_incremental_1_df for outlier detection\n",
    "    local_node1_incremental_1_df['Local_sentiment'] = local_node1_incremental_1_df['Local_sentiment'].replace(sentiment_mapping)\n",
    "    new_row = local_node1_incremental_1_df[['Local_sentiment']].iloc[0].to_dict()\n",
    "    scaler.learn_one(new_row)  # Update the scaler with the new data\n",
    "    new_row_scaled = scaler.transform_one(new_row)\n",
    "    is_outlier = hst.score_one(new_row_scaled) > 0.5  # Threshold can be adjusted\n",
    "\n",
    "    if not is_outlier:\n",
    "        df_input = pd.concat([pred_synth_local_text_df, local_node1_incremental_1_df])\n",
    "    else:\n",
    "        df_input = pred_synth_local_text_df\n",
    "\n",
    "    # After outlier detection, ensure the column name expected by balance_dataset is present\n",
    "    if 'Sentiment' not in df_input.columns and 'Local_sentiment' in df_input.columns:\n",
    "        df_input.rename(columns={'Local_sentiment': 'Sentiment'}, inplace=True)\n",
    "\n",
    "    # Convert numeric Sentiment values back to string labels\n",
    "    bgn_df_input = df_input.copy()\n",
    "    bgn_df_input['Sentiment'] = bgn_df_input['Sentiment'].replace(sentiment_label_mapping)\n",
    "\n",
    "    # Step 1: Check and balance dataset\n",
    "    df = balance_dataset(df_input)\n",
    "\n",
    "    # Step 2: Preprocess text data\n",
    "    df['processed_caption'] = df['Caption'].apply(preprocess_text)\n",
    "    sentences = df['processed_caption'].tolist()\n",
    "    word2vec_model = create_embeddings(sentences)\n",
    "\n",
    "    # Convert sentences to sequences of vectors\n",
    "    vectorized_synth_local = [[word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv] for sentence in sentences]\n",
    "\n",
    "    # Now pad or truncate the sequences\n",
    "    adjusted_sequences = pad_or_truncate_sequences(vectorized_synth_local, fixed_input_size, vector_size)\n",
    "\n",
    "    # Flatten the sequences for input to the neural network\n",
    "    adjusted_sequences = adjusted_sequences.reshape(len(adjusted_sequences), -1)\n",
    "\n",
    "    # Target variable is one-hot encoded and needs to be converted to class indices\n",
    "    # Convert target variable from one-hot encoded to class indices\n",
    "    target = df['Sentiment'].factorize()[0]\n",
    "\n",
    "    # Split dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(adjusted_sequences, target, test_size=0.25, random_state=33)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Ensure this is long type for CrossEntropyLoss\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "    # DataLoader\n",
    "    train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_data, batch_size=64)\n",
    "\n",
    "    # Load model with pretrained parameters\n",
    "    model = SentimentAnalysisModel_global(input_size=modified_input_size)\n",
    "    model.load_state_dict(torch.load(global_parameters_path))\n",
    "    model.train()\n",
    "\n",
    "    # Step 3: Train model with differential privacy\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.1)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    privacy_engine = PrivacyEngine()\n",
    "    model, optimizer, train_loader = privacy_engine.make_private(\n",
    "        module=model,\n",
    "        optimizer=optimizer,\n",
    "        data_loader=train_loader,\n",
    "        noise_multiplier=1.0,\n",
    "        max_grad_norm=1.0\n",
    "    )\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(10):\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Return updated model parameters\n",
    "    return bgn_recommend_df, bgn_df_input, model.state_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\leo_c\\AppData\\Local\\Temp\\ipykernel_16864\\1903831032.py:139: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  pred_synth_local_text_df['Local_sentiment'] = pred_synth_local_text_df['Local_sentiment'].replace(sentiment_mapping)\n",
      "c:\\Users\\leo_c\\anaconda3\\lib\\site-packages\\opacus\\privacy_engine.py:142: UserWarning: Secure RNG turned off. This is perfectly fine for experimentation as it allows for much faster training performance, but remember to turn it on and retrain one last time before production with ``secure_mode`` turned on.\n",
      "  warnings.warn(\n",
      "c:\\Users\\leo_c\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1359: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "recommend_df, incremental_learning_local_dataset_df, updated_parameters = submodel_four('global_parameters.pt', synth_local_text_df, node1_incremental_1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caption</th>\n",
       "      <th>Local_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hands on her face and her mouth</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Caption Local_sentiment\n",
       "3  hands on her face and her mouth            good"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Caption</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>crouching down next to two water bottles</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coat and glass table</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fcx tutorial video 4 - volumetric smoke</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hands on her face and her mouth</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>crouching down next to two water bottles</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the open document in an excel document</td>\n",
       "      <td>good</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Caption Sentiment\n",
       "0  crouching down next to two water bottles   neutral\n",
       "1                      coat and glass table   neutral\n",
       "2   fcx tutorial video 4 - volumetric smoke   neutral\n",
       "3           hands on her face and her mouth      good\n",
       "4  crouching down next to two water bottles   neutral\n",
       "0    the open document in an excel document      good"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incremental_learning_local_dataset_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('_module.fc1.weight',\n",
       "              tensor([[-3.2876e-03, -1.7725e-01, -1.7978e-01,  ..., -1.6747e-01,\n",
       "                       -3.6083e-01, -1.2868e-01],\n",
       "                      [-5.3667e-01,  5.4846e-01, -4.9409e-02,  ...,  4.5761e-04,\n",
       "                        4.9699e-01, -1.0931e-01]])),\n",
       "             ('_module.fc1.bias', tensor([ 0.6339, -0.0688])),\n",
       "             ('_module.fc2.weight',\n",
       "              tensor([[ 0.9948, -0.8232],\n",
       "                      [-0.2900, -0.5801],\n",
       "                      [-0.0239,  0.1580]])),\n",
       "             ('_module.fc2.bias', tensor([-0.0202,  0.6626, -0.3815])),\n",
       "             ('_module.groupnorm.weight', tensor([1.4470, 1.4697])),\n",
       "             ('_module.groupnorm.bias', tensor([0.2886, 0.0109]))])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
